{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sklearn\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2847\n"
     ]
    }
   ],
   "source": [
    "df_Stock = pd.read_csv(\"Desktop/finalproject_training.csv\")\n",
    "comp_list = df_Stock.comp_id.unique()\n",
    "print(len(comp_list))\n",
    "\n",
    "### Create next month return\n",
    "\n",
    "df_Stock[\"m_next_ret\"] = df_Stock.groupby('comp_id')[\"m_ret\"].shift(-1)\n",
    "df_Stock = df_Stock.dropna(subset = [\"m_next_ret\"])\n",
    "\n",
    "### Construct self-defined feature\n",
    "\n",
    "df_Stock[\"h/l\"] = df_Stock[\"m_high_adj\"] / df_Stock[\"m_low_adj\"] - 1\n",
    "df_Stock[\"d/p\"] = df_Stock[\"m_divs\"] / df_Stock[\"close_adj\"]\n",
    "df_Stock[\"log_m_volume_adj\"] = np.log(df_Stock[\"m_volume_adj\"] + 1)\n",
    "df_Stock[\"log_SP500\"] = np.log(df_Stock[\"SP500WeeklyClose\"] + 1)\n",
    "\n",
    "\n",
    "feature_names = [\"m_ret\", \"d/p\", \"h/l\", \"log_m_volume_adj\", \"log_SP500\", \n",
    "                 \"Bullish\", \"Bearish\", \"Bullish8WeekMovAvg\", \n",
    "                 \"epsfxq\",  \"mkvaltq\", \"gsector\"]\n",
    "\n",
    "### Select company based on data availability of feature\n",
    "comp_selection = comp_list\n",
    "for feature in feature_names:\n",
    "    comp_selection = comp_selection[df_Stock.groupby(df_Stock.comp_id, sort = False)[feature]\\\n",
    "        .apply(lambda x: not x.isnull().all())]\n",
    "    df_Stock = df_Stock[df_Stock.comp_id.isin(comp_selection)]\n",
    "    df_Stock[feature] = df_Stock.groupby(df_Stock.comp_id, sort = False)[feature].apply(lambda x: x.ffill().bfill()) \n",
    "\n",
    "### Create Dummy Variables for feature gsector\n",
    "\n",
    "non_dummy_cols = df_Stock.columns\n",
    "df_Stock = pd.get_dummies(df_Stock, columns = [\"gsector\"])\n",
    "dummy_cols = list(set(df_Stock.columns) - set(non_dummy_cols))\n",
    "\n",
    "feature_names = [\"m_ret\", \"d/p\", \"h/l\", \"log_m_volume_adj\", \"log_SP500\", \n",
    "                 \"Bullish\", \"Bearish\", \"Bullish8WeekMovAvg\", \n",
    "                 \"epsfxq\",  \"mkvaltq\"] + dummy_cols\n",
    "\n",
    "### Construct training dataset\n",
    "\n",
    "X_ret = df_Stock.loc[:, feature_names]\n",
    "y_ret = df_Stock.loc[:, \"m_next_ret\"]\n",
    "\n",
    "X_gof = df_Stock.loc[:, feature_names]\n",
    "y_gof = ((1 + np.sign(y_ret)) / 2).astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct an iterable of training-test splits for Cross Validation via ShufflieSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cross_validation_shufflesplit = ShuffleSplit(n_splits = 10, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Score: [-0.59574363 -0.59146958 -0.61635034 -0.6230525  -0.57982949 -0.55932197\n",
      " -0.56718896 -0.63691172 -0.58772993 -0.58080423 -0.60125917 -0.5596763 ]\n",
      "Standard Deviation of Test Score: [0.14592403 0.21250379 0.14788211 0.18761797 0.16872052 0.1694852\n",
      " 0.17916905 0.18372441 0.16693642 0.18245085 0.19069797 0.17654757]\n",
      "Optimal Model Parameters: {'baggingregressor__base_estimator__hidden_layer_sizes': (6, 3, 2), 'baggingregressor__n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "base_MLP_regressor = MLPRegressor(max_iter = 1000)\n",
    "MLP_regressor_bagging = BaggingRegressor(base_estimator = base_MLP_regressor)\n",
    "MLP_regressor_bagging_pipeline = make_pipeline(StandardScaler(), MLP_regressor_bagging)\n",
    "\n",
    "MLP_regressor_bagging_param_grid = dict(baggingregressor__base_estimator__hidden_layer_sizes = [(5, 2), (6, 3, 2), (8, 4, 2)],\n",
    "                                        baggingregressor__n_estimators = [5, 10, 15, 20])\n",
    "\n",
    "MLP_regressor_bagging_grid_search = GridSearchCV(MLP_regressor_bagging_pipeline,\n",
    "                                                 param_grid = MLP_regressor_bagging_param_grid,\n",
    "                                                 cv = cross_validation_shufflesplit,\n",
    "                                                 scoring = \"neg_mean_squared_error\")\n",
    "                                                \n",
    "MLP_regressor_bagging_grid_search.fit(X_ret, y_ret)\n",
    "\n",
    "means = MLP_regressor_bagging_grid_search.cv_results_[\"mean_test_score\"]\n",
    "stds = MLP_regressor_bagging_grid_search.cv_results_[\"std_test_score\"]\n",
    "\n",
    "\n",
    "print(\"Mean Test Score:\", means)\n",
    "print(\"Standard Deviation of Test Score:\",  stds)\n",
    "print('Optimal Model Parameters: ' + str(MLP_regressor_bagging_grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Score: [0.63219946 0.63383072 0.63567695 0.63584977 0.63912915 0.64155707\n",
      " 0.64267409 0.6423158  0.64383325 0.64732339 0.64837717 0.64990727]\n",
      "Standard Deviation of Test Score: [0.00364989 0.0041399  0.00428267 0.00314623 0.00459667 0.00450393\n",
      " 0.00317144 0.00227266 0.0039586  0.00436569 0.00251905 0.00337549]\n",
      "Optimal Model Parameters: {'baggingclassifier__base_estimator__hidden_layer_sizes': (8, 4, 2), 'baggingclassifier__n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "base_MLP_classifier = MLPClassifier(max_iter = 1000)\n",
    "MLP_classifier_bagging = BaggingClassifier(base_estimator = base_MLP_classifier)\n",
    "MLP_classifier_bagging_pipeline = make_pipeline(StandardScaler(), MLP_classifier_bagging)\n",
    "\n",
    "MLP_classifier_bagging_param_grid = dict(baggingclassifier__base_estimator__hidden_layer_sizes = [(5, 2), (6, 3, 2), (8, 4, 2)],\n",
    "                                        baggingclassifier__n_estimators = [5, 10, 15, 20])\n",
    "\n",
    "MLP_classifier_bagging_grid_search = GridSearchCV(MLP_classifier_bagging_pipeline,\n",
    "                                                 param_grid = MLP_classifier_bagging_param_grid,\n",
    "                                                 cv = cross_validation_shufflesplit,\n",
    "                                                 scoring = \"accuracy\",\n",
    "                                                 n_jobs = -1)\n",
    "                                                \n",
    "MLP_classifier_bagging_grid_search.fit(X_gof, y_gof)\n",
    "\n",
    "means = MLP_classifier_bagging_grid_search.cv_results_[\"mean_test_score\"]\n",
    "stds = MLP_classifier_bagging_grid_search.cv_results_[\"std_test_score\"]\n",
    "\n",
    "\n",
    "print(\"Mean Test Score:\", means)\n",
    "print(\"Standard Deviation of Test Score:\",  stds)\n",
    "print('Optimal Model Parameters: ' + str(MLP_classifier_bagging_grid_search.best_params_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
